{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffamilies = [\n",
    "    lambda k: lambda x: x[0, 0]**2 + k * x[0, 0] * x[0, 1] + x[0,1]**2,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial(f, x, i=[0], dx=1e-6):\n",
    "    \"\"\"Computes the i-th partial derivative of f at point x.\n",
    "    \n",
    "    Args:\n",
    "        f: objective function.\n",
    "        x: point at which partial derivative is computed.\n",
    "        i: list of coordinates along which derivative is computed (differentiates successively once per coordinate).\n",
    "        dx: slack for finite difference.\n",
    "        \n",
    "    Output:\n",
    "        (float)\n",
    "    \"\"\"\n",
    "    if not i:\n",
    "        return f(x)\n",
    "    x = x.reshape(1, -1)\n",
    "    h = np.zeros(x.shape)\n",
    "    h[0, i[0]] = dx\n",
    "    p1 = partial(f, x + h, i[1:], dx)\n",
    "    p2 = partial(f, x - h, i[1:], dx)\n",
    "    return (p1 - p2) / (2*dx)\n",
    "\n",
    "def deriv1(f, x, i, dx=1e-6):\n",
    "    return partial(f, x, [i], dx)\n",
    "\n",
    "def deriv2(f, x, i, j, dx=1e-6):\n",
    "    return partial(f, x, [i, j], dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f, x, dx=1e-6):\n",
    "    \"\"\"Computes gradient of f at point x.\n",
    "    \n",
    "    Args:\n",
    "        f: objective function.\n",
    "        x: point at which gradient is computed.\n",
    "        dx: slack for finite difference of partial derivatives.\n",
    "        \n",
    "    Output:\n",
    "        (ndarray) of size domain of f.\n",
    "    \"\"\"\n",
    "    x = x.reshape(1, -1)\n",
    "    dim = x.shape[1]\n",
    "    return np.array([deriv1(f, x, i, dx) for i in range(dim)]).reshape(1, -1)\n",
    "\n",
    "def hessian(f ,x, dx=1e-6):\n",
    "    \"\"\"Computes hessian of f at point x.\n",
    "    \n",
    "    Args:\n",
    "        f: objective function.\n",
    "        x: point at which hessian is computed.\n",
    "        dx: slack for finite difference of partial derivatives.\n",
    "        \n",
    "    Output:\n",
    "        (ndarray) of square shape and size domain of f.\n",
    "    \"\"\"\n",
    "    x = x.reshape(1, -1)\n",
    "    dim = x.shape[1]\n",
    "    line = lambda i: np.array([deriv2(f, x, i, j, dx) for j in range(dim)])\n",
    "    return np.array([line(i) for i in range(dim)])\n",
    "\n",
    "def condition_number(f, x):\n",
    "    H = hessian(f, x)\n",
    "    eivals, _ = np.linalg.eig(H)\n",
    "    return abs(max(eivals) / min(eivals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicUpdater:\n",
    "    def __init__(self, rate=0.01, tol=1e-6):\n",
    "        self.rate = rate\n",
    "        self.tol = tol\n",
    "\n",
    "    def __call__(self, f, x, reset):\n",
    "        return self.rate * -gradient(f, x, self.tol)\n",
    "\n",
    "class GD():\n",
    "    \"\"\"Gradient Descent Object.\n",
    "    \n",
    "    Implements gradient descent aiming to compute optimal objective \n",
    "    value of convex functions and local optimal ones of none \n",
    "    convex functions.\n",
    "    \"\"\"    \n",
    "    def __init__(self, delta=None, decay=None, tol=1e-6, max_iter=1000):\n",
    "        \"\"\"\n",
    "        Instantiates a GD object.\n",
    "    \n",
    "        Attributes:\n",
    "        delta: function computing descent update vector (direction + step)\n",
    "        decay: function computing decay\n",
    "        tol: slack tolerance.\n",
    "        max_iter: upper bound on number of iterations.\n",
    "        \"\"\"\n",
    "        self.delta = delta if delta is not None else ClassicUpdater(0.01, tol)\n",
    "        self.decay = decay if decay is not None else lambda f, x: np.linalg.norm(gradient(f, x, tol))\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.grad = gradient\n",
    "    \n",
    "    def __call__(self, x, f):\n",
    "        \"\"\"Calling gradient descent object with specific starting point and optimal function.\n",
    "        \n",
    "        Args:\n",
    "            x: initial starting point for descent.\n",
    "            f: objective function of optimisation problem.\n",
    "        \n",
    "        Output:\n",
    "            (float) sub-optimal value up to tolerance if execution is proper.\n",
    "            (ndarray) list of gradient descent iterates.\n",
    "        \"\"\"\n",
    "        # Helper functions\n",
    "        compute_delta = lambda x, reset=False: self.delta(f, x, reset)\n",
    "        compute_decay = lambda x: self.decay(f, x)\n",
    "        \n",
    "        # Start\n",
    "        x = x.reshape(1, -1)\n",
    "        n_iter = 0\n",
    "        iters, iters_dir = x, compute_delta(x, reset=True)\n",
    "        decay = compute_decay(x)\n",
    "        while decay > self.tol and n_iter < self.max_iter:\n",
    "            ## Decide on direction\n",
    "            delta = compute_delta(x)\n",
    "            ## Update iterate\n",
    "            x = x + delta\n",
    "            ## Store on-going data\n",
    "            iters = np.vstack([iters, x])\n",
    "            iters_dir = np.vstack([iters_dir, delta])\n",
    "            ## Update decay\n",
    "            decay = compute_decay(x)\n",
    "            ## Update iteration number\n",
    "            n_iter += 1\n",
    "\n",
    "        # Display results\n",
    "        msg = \" Iteration nu. = {}\\n approx. = {}\\n ob value = {}\\n and decay = {}.\"\n",
    "        print(msg.format(n_iter, x.flatten(), f(x), decay))\n",
    "        if decay > self.tol:\n",
    "            warnings.warn(\"Decay didn't get under tolerance rate.\", RuntimeWarning)\n",
    "        return (x, iters, iters_dir, n_iter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration nu. = 878\n",
      " approx. = [2.96883265e-07 3.95844353e-07]\n",
      " ob value = 2.4483242466671736e-13\n",
      " and decay = 9.896108824517185e-07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DG_classic = GD()\n",
    "DG_classic(np.r_[15, 20], ffamilies[0](0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NesterovUpdater:\n",
    "    def __init__(self, rate=0.01, momentum=0.9, tol=1e-6):\n",
    "        self.rate = rate\n",
    "        self.momentum = momentum\n",
    "        self.tol = tol\n",
    "    \n",
    "    def __call__(self, f, x, reset):\n",
    "        x = x.reshape(1, -1)\n",
    "        if reset is True:\n",
    "            self.m_vect = np.zeros(x.shape[1])\n",
    "        mv = self.momentum * self.m_vect\n",
    "        self.m_vect = mv - self.rate * gradient(f, x + mv, self.tol)\n",
    "        return self.m_vect\n",
    "\n",
    "class AdamUpdater:\n",
    "    def __init__(self, rate=0.01, beta1=0.9, beta2=0.999, tol=1e-6):\n",
    "        self.rate = rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.tol = tol\n",
    "\n",
    "    def __call__(self, f, x, reset):\n",
    "        x = x.reshape(1, -1)\n",
    "        if reset is True:\n",
    "            self.m_vect = np.zeros(x.shape[1])\n",
    "            self.s_vect = np.zeros(x.shape[1])\n",
    "        gf = gradient(f, x, self.tol)\n",
    "        m = self.beta1 * self.m_vect - (1 - self.beta1) * gf\n",
    "        s = self.beta2 * self.s_vect + (1 - self.beta2) * np.cross(gf, gf)\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration nu. = 252\n",
      " approx. = [3.17311626e-08 4.23082156e-08]\n",
      " ob value = 2.7968517893163512e-15\n",
      " and decay = 1.0577054011994732e-07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DG_nesterov = GD(NesterovUpdater())\n",
    "DG_nesterov(np.r_[15, 20], ffamilies[0](0))\n",
    "DG_adam = GD(AdamUpdater())\n",
    "DG_adam(np.r_[15, 20], ffamilies[0](0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
